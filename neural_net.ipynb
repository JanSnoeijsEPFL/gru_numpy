{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "#import framework\n",
    "#import implementations as func\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import scipy.signal as scp\n",
    "import tensorflow as tf\n",
    "import target_data_gen\n",
    "from target_data_gen import get_sizes\n",
    "from target_data_gen import target_gen\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():  \n",
    "  \n",
    "    # initialization function\n",
    "    def __init__(self, inputs, outputs):\n",
    "        print(\"creating Linear layer\")\n",
    "        lim = np.sqrt(6/(inputs+outputs))\n",
    "        self.W = np.random.uniform(-lim,lim,(inputs+1, outputs))\n",
    "        self.W[0,:].fill(0)\n",
    "        self.dW = np.zeros((inputs+1, outputs))\n",
    "        self.Linear_updateW = Optimizer(self.W)\n",
    "    # forward pass calculation\n",
    "    # receives input tensor of shape : [nb_samples x insize]\n",
    "    def forward(self,X): \n",
    "        # [nb_samples x insize] x [insize x outsize] + [nb_samples x outsize] => [nb_samples x outsize]\n",
    "        H = X @ self.W\n",
    "        return H\n",
    "    \n",
    "    # backward pass calculation\n",
    "    # receives gradient of the next layer\n",
    "    # returns gradient with respect to the input. \n",
    "    # returns derivative with respect to the weights.\n",
    "    def backward(self,dH, X):\n",
    "        # matrix multiplication [nb_samples, outsize]*[insize, outsize]^t => [nb_samples x insize]\n",
    "        # sum over samples for parameters (normal Gradient Descent. If divided into batches -> SGD)\n",
    "        dX = dH @ np.transpose(self.W)\n",
    "        #print(dH, self.dW)\n",
    "        self.dW = np.transpose(X) @ dH\n",
    "        self.W = self.Linear_updateW.adam_update(self.W, self.dW)\n",
    "        return dX, self.dW\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonal_initializer(shape):\n",
    "    num_rows = 1\n",
    "    for dim in shape[:-1]:\n",
    "        num_rows *= dim\n",
    "    num_cols = shape[-1]\n",
    "    flat_shape = (num_rows, num_cols)\n",
    "    a = np.random.normal(0.0, 1.0, flat_shape)\n",
    "    u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "    # Pick the one with the correct shape.\n",
    "    q = u if u.shape == flat_shape else v\n",
    "    q = q.reshape(shape)\n",
    "    return q[:shape[0], :shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU():\n",
    "    #Wz, Wr, Wh, Uz, Ur, Uh = np.array()\n",
    "    #z, r, h, s = np.array()\n",
    "    \n",
    "    def __init__(self,sequences, timesteps, inputs, outputs):\n",
    "        print(\"creating GRU layer\")\n",
    "        lim_w = np.sqrt(6/(inputs+outputs))\n",
    "        lim_u = np.sqrt(6/(2*outputs))\n",
    "        lim_wlin = np.sqrt(6/(outputs))\n",
    "        self.Wz = np.random.uniform(-lim_w,lim_w,(inputs, outputs))\n",
    "        self.Wr = np.random.uniform(-lim_w,lim_w,(inputs, outputs))\n",
    "        self.Wh = np.random.uniform(-lim_w,lim_w,(inputs, outputs))\n",
    "        self.Bz = np.zeros((1, outputs))\n",
    "        self.Br = np.zeros((1, outputs))\n",
    "        self.Bh = np.zeros((1, outputs))\n",
    "        self.Wlin = np.random.uniform(-lim_wlin, lim_wlin, (outputs, 1))\n",
    "        self.Blin = np.zeros((1,1))\n",
    "        print(self.Wz.shape)\n",
    "        self.y, self.dy = np.zeros((sequences, 1)) ,np.zeros((sequences,1))\n",
    "        #self.Uz, self.Ur, self.Uh = np.random.uniform(-lim_u,lim_u,(outputs, outputs)), np.random.uniform(-lim_u,lim_u,(outputs, outputs)), np.random.uniform(-lim_u,lim_u,(outputs, outputs))\n",
    "        self.Uz = orthogonal_initializer((outputs, outputs))\n",
    "        self.Ur = orthogonal_initializer((outputs, outputs))\n",
    "        self.Uh = orthogonal_initializer((outputs, outputs))\n",
    "        self.z, self.r = np.zeros((sequences,timesteps,outputs)),np.zeros((sequences,timesteps,outputs))\n",
    "        self.h, self.s = np.zeros((sequences,timesteps,outputs)),np.zeros((sequences,timesteps,outputs))\n",
    "        print(self.s.shape)\n",
    "        self.dWlin = np.zeros((outputs,1))\n",
    "        self.GRU_updateBz = Optimizer(self.Bz)\n",
    "        self.GRU_updateBr = Optimizer(self.Br)\n",
    "        self.GRU_updateBh = Optimizer(self.Bh)\n",
    "        self.GRU_updateWz = Optimizer(self.Wz)\n",
    "        self.GRU_updateWr = Optimizer(self.Wr)\n",
    "        self.GRU_updateWh = Optimizer(self.Wh)\n",
    "        self.GRU_updateUz = Optimizer(self.Uz)\n",
    "        self.GRU_updateUr = Optimizer(self.Ur)\n",
    "        self.GRU_updateUh = Optimizer(self.Uh)\n",
    "        self.Lin_updateW = Optimizer(self.Wlin)\n",
    "        self.Lin_updateB = Optimizer(self.Blin)\n",
    "        #self.s_aug = np.zeros((sequences, timesteps, outputs+1))\n",
    "        #self.s_aug[:,:,0].fill(1) # bias\n",
    "    def forward(self,X):\n",
    "        # initialize\n",
    "        # first iteration\n",
    "        #print(self.z.shape, X.shape)\n",
    "        outputs = self.s.shape[2]\n",
    "        \n",
    "        self.z[:,0,:] = sigmoid(X[:,0,:] @ self.Wz + self.Bz) #[seq * time * OUT] = [seq * time * IN] @ [IN * OUT]\n",
    "        self.r[:,0,:] = sigmoid(X[:,0,:] @ self.Wr + self.Br)\n",
    "        self.h[:,0,:] = tanh(X[:,0,:] @ self.Wh + self.Bh)\n",
    "        self.s[:,0,:] = (1-self.z[:,0,:])*self.h[:,0,:]\n",
    "        for t in range(1, X.shape[1]):\n",
    "            self.z[:,t,:] = sigmoid(X[:,t,:] @ self.Wz + self.s[:,t-1,:] @ self.Uz + self.Bz) #[samples,outputs]+[1, outputs]\n",
    "            self.r[:,t,:] = sigmoid(X[:,t,:] @ self.Wr + self.s[:,t-1,:] @ self.Ur + self.Br)\n",
    "            self.h[:,t,:] = tanh(X[:,t,:] @ self.Wh + (self.r[:,t,:] *self.s[:,t-1,:]) @ self.Uh + self.Bh)\n",
    "            self.s[:,t,:] = self.z[:,t,:]*self.s[:,t-1,:] + (1-self.z[:,t,:])*self.h[:,t,:]\n",
    "            #self.s_aug [:,t,1:outputs+1] = self.s[:,t,:]\n",
    "        self.y = sigmoid(self.s[:,self.s.shape[1]-1,:] @ self.Wlin + self.Blin)\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self,dy, X):\n",
    "        inputs = X.shape[2]\n",
    "        outputs = self.s.shape[2]\n",
    "        dsnext = np.zeros_like(self.s[:,0,:])\n",
    "        dsnext = dy @ np.transpose(self.Wlin)\n",
    "        dX = np.zeros((X.shape))\n",
    "        dWlin = np.transpose(self.s[:,self.s.shape[1]-1,:])@dy\n",
    "        dBlin = np.sum(dy, axis = 0).reshape(1,-1)\n",
    "        dWz, dWr, dWh = np.zeros((inputs, outputs)), np.zeros((inputs, outputs)), np.zeros((inputs, outputs))\n",
    "        dUz, dUr, dUh = np.zeros((outputs, outputs)), np.zeros((outputs, outputs)), np.zeros((outputs, outputs))\n",
    "        dBz, dBr, dBh = np.zeros((1, outputs)), np.zeros((1, outputs)), np.zeros((1, outputs))\n",
    "        for t in reversed(range(X.shape[1])):\n",
    "            #print(ds.shape, self.z.shape)\n",
    "            \n",
    "            \n",
    "            ds = dsnext\n",
    "            dh = ds*(1-self.z[:,t,:])\n",
    "            dh_l = dh*tanh(self.h[:,t,:], deriv=True)\n",
    "            #print(\"GRU backward\", dh_l.shape)\n",
    "            dWh += np.transpose(X[:,t,:]) @ dh_l # [1 x IN].T @ [1 x OUT]\n",
    "           \n",
    "            #print(self.dWh.shape)\n",
    "            dUh += np.transpose(self.r[:,t,:]*self.s[:,t-1,:]) @ dh_l # [1 x OUT].T @ ([1 x OUT] * [1 x OUT])\n",
    "            dBh += np.sum(dh_l, axis = 0).reshape(1,-1)\n",
    "            #drsp = dh_l @ np.transpose(Uh)\n",
    "            \n",
    "            drsp = dh_l @ np.transpose(self.Uh)# [1 x OUT] @ [OUTin x OUT].T\n",
    "           # dr = dh_l * (self.s[:,t-1,:] @ self.Uh) #[1 x OUT] = [ 1 x OUT]*([1 x OUTin] @ [OUTin x OUT])\n",
    "            dr = drsp * self.s[:,t-1,:]\n",
    "            dr_l = dr * sigmoid(self.r[:,t,:], deriv=True) # replace by sigmoid\n",
    "            \n",
    "            dWr += np.transpose(X[:,t,:]) @ dr_l # [ IN x OUT] = [1 x IN].T @ [1 x OUT]\n",
    "            dUr += np.transpose(self.s[:,t-1,:]) @ dr_l # [OUTin x OUT ] = [ 1 x OUTin].T @ [ 1 x OUT]\n",
    "            dBr += np.sum(dr_l, axis = 0).reshape(1,-1)\n",
    "            \n",
    "            dz = (self.s[:,t-1,:]-self.h[:,t,:]) * dh  # [1 x OUT] = ( [1 x OUT] - [1 x OUTin] ) * [1 x OUT]\n",
    "            dz_l = dz * sigmoid(self.z[:,t,:], deriv=True)\n",
    "            \n",
    "            dWz += np.transpose(X[:,t,:]) @ dz_l\n",
    "            dUz += np.transpose(self.s[:,t-1,:]) @ dz_l\n",
    "            dBz += np.sum(dz_l, axis = 0).reshape(1,-1)\n",
    "            \n",
    "            # calculate gradient w.r.t s[t-1]\n",
    "            ds_fz_inner = dz_l @ np.transpose(self.Uh) #  [1 x OUTin] =  [1 x OUT] @ [OUTin x OUT].T\n",
    "            ds_fz = ds * (1-self.z[:,t,:]) # [1 x OUTin] = [1 x OUT] * [1 x OUT]\n",
    "            ds_fh = drsp * self.r[:,t,:] # [1 x OUTin] = [1 x OUT] * [1 x OUT]\n",
    "            ds_fr = dr_l @ np.transpose(self.Ur)\n",
    "            \n",
    "            dsnext = ds_fz_inner + ds_fz + ds_fh + ds_fr\n",
    "            for n in range(0, t+1):\n",
    "                dX[:,n,:]+=dh_l @ np.transpose(self.Wh) + dr_l @ np.transpose(self.Wr) + dz_l @ np.transpose(self.Wz)\n",
    "            \n",
    "            #update weights\n",
    "        self.Wz = self.GRU_updateWz.adam_update(self.Wz, dWz)\n",
    "        self.Wr = self.GRU_updateWr.adam_update(self.Wr, dWr)\n",
    "        self.Wh = self.GRU_updateWh.adam_update(self.Wh, dWh)\n",
    "        self.Uz = self.GRU_updateUz.adam_update(self.Uz, dUz)\n",
    "        self.Ur = self.GRU_updateUr.adam_update(self.Ur, dUr)\n",
    "        self.Uh = self.GRU_updateUh.adam_update(self.Uh, dUh) \n",
    "        self.Bz = self.GRU_updateBz.adam_update(self.Bz, dBz)\n",
    "        self.Br = self.GRU_updateBr.adam_update(self.Br, dBr)\n",
    "        self.Bh = self.GRU_updateBh.adam_update(self.Bh, dBh)\n",
    "        self.Wlin = self.Lin_updateW.adam_update(self.Wlin, dWlin)\n",
    "        self.Blin = self.Lin_updateB.adam_update(self.Blin, dBlin)\n",
    "        return ds, dX\n",
    "    \n",
    "    def change_input_size(self, sequences, timesteps,outputs):\n",
    "        self.z, self.r = np.zeros((sequences,timesteps,outputs)),np.zeros((sequences,timesteps,outputs))\n",
    "        self.h, self.s = np.zeros((sequences,timesteps,outputs)),np.zeros((sequences,timesteps,outputs))\n",
    "        self.s_aug = np.zeros((sequences, timesteps, outputs+1))\n",
    "        self.y = np.zeros((sequences, timesteps, 1))\n",
    "    def get_parameters(self):\n",
    "        return self.Wz, self.dWz,self.Wr, self.dWr,self.Wh, self.dWh,self.Uz, self.dUz,self.Ur, self.dUr,self.Uh, self.dUh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 4],\n",
       "       [4, 6],\n",
       "       [6, 8]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array ([[1,2],[3,4],[5,6]])\n",
    "B = np.array([[1,2]])\n",
    "A+B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[4 6]\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "[[ 4 12]\n",
      " [12 24]]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([[1,2],[3,4]])\n",
    "B=np.array([[1,2],[3,4]])\n",
    "print(A)\n",
    "C = np.sum(A,axis=0)\n",
    "print(C)\n",
    "print(B)\n",
    "print(C.reshape(1,-1)*B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4 5 6 7 8 9]\n",
      " [1 2 3 4 5 6 7 8 9]]\n",
      "creating GRU layer\n",
      "(9, 5)\n",
      "(1, 2, 5) (1, 2, 9)\n",
      "[[[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([np.arange(1,10), np.arange(1,10)])\n",
    "X.reshape(1,2,9)\n",
    "print(X)\n",
    "GRU_layer_0 = GRU(sequences=1, timesteps=2, inputs=9, outputs=5)\n",
    "s = GRU_layer_0.forward(X.reshape(1,2,9))\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input, deriv=False): #requires output of forward prop for derivativef\n",
    "    if deriv:\n",
    "        return input*(1-input)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def hard_sigmoid(input, deriv = False): #requires input of forward prop for derivative.\n",
    "    if deriv:\n",
    "        temp = np.copy(input)\n",
    "        temp[input <= -2] = 0\n",
    "        temp[input > -2] = 0.25*input\n",
    "        temp[input >= 2] = 0\n",
    "        return temp\n",
    "    else:\n",
    "        return np.maximum(0, np.minimum(1, (input + 2) / 4))\n",
    "def tanh(input, deriv=False): #requires output of forward prop for derivative\n",
    "    if deriv:\n",
    "        return 1 - input ** 2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "def reLU(input, deriv=False): #requires input of forward prop for derivative\n",
    "    if deriv:\n",
    "        output = np.copy(input)\n",
    "        output[input > 0.0 ] = 1.0\n",
    "        output[input <= 0.0] = 0.0\n",
    "        return output\n",
    "    else:\n",
    "        output = np.copy(input)\n",
    "        output[input < 0.0 ] = 0.0\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "input_shape = (32,10,2)\n",
    "X=np.zeros((input_shape))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D():\n",
    "    height = 0\n",
    "    width = 0\n",
    "    nb_seq = 0\n",
    "    timesteps = 0\n",
    "    new_height = 0\n",
    "    new_width = 0\n",
    "    K = 0\n",
    "    M = 0\n",
    "    N = 0\n",
    "    def __init__(self, kernel_height, kernel_width, filters):\n",
    "        self.W = np.random.uniform(-1,1,(kernel_height, kernel_width, filters))\n",
    "        self.B = np.zeros((1, filters))\n",
    "        self.dW = np.zeros((kernel_height, kernel_width, filters))\n",
    "        self.dB = np.zeros((1, filters))\n",
    "        print('Creating Conv2D layer')\n",
    "        self.Conv2D_updateW = Optimizer(self.W)\n",
    "        self.Conv2D_updateB = Optimizer(self.B)\n",
    "    # W.shape : [kernel_height, kernel_width, nb_filters]\n",
    "    # X.shape : [samples, timesteps, height, width, 1]\n",
    "    # B.shape : [1, nb_filters]\n",
    "    def forward(self, X):\n",
    "        #print(\"conv2d Xshape\", X.shape)\n",
    "        self.height = X.shape[2]\n",
    "        self.width = X.shape[3]\n",
    "        self.M = self.W.shape[0]\n",
    "        self.N = self.W.shape[1]\n",
    "        self.K = self.W.shape[2]\n",
    "        self.nb_seq = X.shape[0]\n",
    "        self.timesteps = X.shape[1]\n",
    "        #print(self.timesteps)\n",
    "        #compute new dimensions\n",
    "        self.new_height = self.height - self.M + 1\n",
    "        self.new_width = self.width - self.N + 1\n",
    "        \n",
    "       # print(new_height, new_width)\n",
    "        h = np.zeros((self.nb_seq, self.timesteps, self.new_height, self.new_width, self.K))\n",
    "        for k in range(self.K):\n",
    "            for i in range(self.new_height):\n",
    "                for j in range(self.new_width):\n",
    "                    h[:,:,i,j,k]=np.sum(X[:,:,i:i+self.M, j:j+self.N,0]*self.W[:,:,k], axis =(2,3))+self.B[0,k]\n",
    "        return h\n",
    "    # dH has dimensions of H which means in case of X.shape=[3,3,1], W.shape=[2,2,1] => H.shape=[2,2,1]\n",
    "    # dX should have the same shape as X, i.e dX.shape=[3,3,1] = dH conv2D flipped W 'FULL'\n",
    "    def backward(self, dH, X):\n",
    "        # dw is the same operation as in forward propagation. \n",
    "        # no need to compute dX in our case because the conv2D layer is the first layer and means the end of backprop. algorithm.\n",
    "        for k in range(self.K):\n",
    "            self.dB[:,k] = np.sum(dH[:,:,:,:,k])\n",
    "            for i in range(self.M):\n",
    "                for j in range(self.N):\n",
    "                    #average over all the sequences and timesteps\n",
    "                    self.dW[i,j,k]=np.sum(X[:,:,i:i+self.new_height, j:j+self.new_width,0]*dH[:,:,:,:,k])\n",
    "        self.W = self.Conv2D_updateW.adam_update(self.W, self.dW)\n",
    "        self.B = self.Conv2D_updateB.adam_update(self.B, self.dB)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 1)\n",
      "(3, 3, 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'kernel_height', 'kernel_width', and 'filters'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-14480a324581>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mconv_layer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_layer1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'kernel_height', 'kernel_width', and 'filters'"
     ]
    }
   ],
   "source": [
    "#test for conv2d\n",
    "X=np.array([[3.0,5,6], [1,2,4], [2,3,5]]).reshape(3,3,1)\n",
    "W=np.array([[2,1.0],[1,4]]).reshape(-1,2,1)\n",
    "#W = np.rot90(W,2)\n",
    "print(W.shape)\n",
    "B=np.array([0.5]).reshape(-1,1)\n",
    "print(X.shape)\n",
    "conv_layer1 = Conv2D()\n",
    "H = conv_layer1.forward(X, W, B)\n",
    "print(H, H.shape)\n",
    "print(H[1,0,0])\n",
    "\n",
    "# check against scp : kernel is flipped here.\n",
    "X_scp = X.reshape(-1,3)\n",
    "W_scp = W.reshape(-1,2)\n",
    "H_scp = scp.convolve2d(X_scp, W_scp, mode = 'valid')\n",
    "#print(H_scp)\n",
    "#print(X[1,0,0])\n",
    "\n",
    "#check against tensorflow\n",
    "\n",
    "X_tf = X.reshape(1,3,3,1)\n",
    "W_tf = W.reshape(2,2,1,1)\n",
    "sess = tf.Session()\n",
    "H_tf = sess.run(tf.nn.conv2d(X_tf, W_tf, strides = (1,1,1,1), padding = 'VALID'))\n",
    "sess.run(tf.Print(H_tf, [H_tf]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D():\n",
    "    height = 0\n",
    "    width = 0\n",
    "    M = 0\n",
    "    N = 0\n",
    "    K = 0\n",
    "    nb_seq = 0\n",
    "    timesteps = 0\n",
    "    def __init__(self):\n",
    "        print('creating 2D Max pooling layer')\n",
    "    \n",
    "    def forward(self, X, pool):\n",
    "        self.height = X.shape[2]\n",
    "        self.width = X.shape[3]\n",
    "        self.M = pool[0]\n",
    "        self.N = pool[1]\n",
    "        self.K = X.shape[4]\n",
    "        self.nb_seq = X.shape[0]\n",
    "        self.nb_timesteps = X.shape[1]\n",
    "        if self.height%2 == 1:\n",
    "            X = np.delete(X, obj=self.height-1, axis=2)\n",
    "            self.height = X.shape[2]\n",
    "        if self.width%2 == 1:\n",
    "            X = np.delete(X, obj=self.width-1, axis=3)\n",
    "            self.width = X.shape[3]\n",
    "        X_argmax = np.copy(X)\n",
    "       \n",
    "        #print(self.M, self.N)\n",
    "        #compute new sizes\n",
    "        new_height = int(self.height/self.M)\n",
    "        new_width = int(self.width/self.N)\n",
    "        H = np.zeros((self.nb_seq, self.nb_timesteps, new_height, new_width, self.K))\n",
    "        #print(H.shape)\n",
    "       \n",
    "        #start pooling\n",
    "        for k in range(self.K):\n",
    "            for i in range(0,self.height,self.M):\n",
    "                for j in range(0,self.width,self.N): #genericity loss here, only valid with kernel width = 2.\n",
    "                    X_temp = X[:,:,i:i+self.M, j:j+self.N, k].reshape(self.nb_seq, self.nb_timesteps, self.M*self.N,1)\n",
    "                    H[:,:,int(i/self.M), int(j/self.N), k] = np.amax(X[:,:,i:i+self.M, j:j+self.N, k], axis=(2,3)) \n",
    "                    X_argmax[:,:, i, j, k] = (np.argmax(X_temp, axis = 2)).reshape(self.nb_seq, self.nb_timesteps) \n",
    "                    X_argmax[:,:, i+self.M-1, j, k] = (np.argmax(X_temp, axis = 2)).reshape(self.nb_seq, self.nb_timesteps) \n",
    "                    X_argmax[:,:, i, j+self.N-1, k] = (np.argmax(X_temp, axis = 2)).reshape(self.nb_seq, self.nb_timesteps) \n",
    "                    X_argmax[:,:, i+self.M-1, j+self.N-1, k] = (np.argmax(X_temp, axis = 2)).reshape(self.nb_seq, self.nb_timesteps) \n",
    "        return H, X_argmax\n",
    "        \n",
    "    def backward(self, X_argmax, dH):\n",
    "        dX = np.zeros((X_argmax.shape))\n",
    "        for k in range(self.K):\n",
    "            for i in range(0, self.height, self.M):\n",
    "                for j in range(0,self.width, self.N):\n",
    "                    dX[:,:, i:i+self.M,j:j+self.N,k] = self.norm_argmax(X_argmax[:,:,i:i+self.M, j:j+self.N, k], dH[:,:,int(i/self.M), int(j/self.N), k])       \n",
    "        return dX\n",
    "                                                                \n",
    "    def norm_argmax(self, X, dH):\n",
    "        #print(\"X_argmax shape maxpool = \", X.shape)\n",
    "        h = X.shape[2]\n",
    "        w = X.shape[3]\n",
    "        I = np.array(np.arange(X.shape[2]+X.shape[3]))\n",
    "        Ibig = np.zeros_like(X)\n",
    "        #print(\"Maxpoolbackward:\", X.shape, Ibig.shape, I.shape)\n",
    "        Ibig = np.tile(I,(X.shape[0], X.shape[1],1))\n",
    "        #print(\"ibig\", Ibig.shape)\n",
    "        Xresh = X.reshape(X.shape[0], X.shape[1], Ibig.shape[2]) # multidimensional\n",
    "        diff = (Xresh-Ibig).astype(int)\n",
    "        #duplicate dH x 4\n",
    "        dH_dupl = np.zeros_like(X)\n",
    "        for m in range(2):\n",
    "            for n in range(2):\n",
    "                dH_dupl[:,:,m,n] = dH\n",
    "        # reshape to make boolean assignment\n",
    "        Xresh = Xresh.flatten()\n",
    "        diff = diff.flatten()\n",
    "        dHflat = dH_dupl.flatten()\n",
    "        #print(\"dH shape\", dH.shape)\n",
    "        Xresh[diff != 0] = -1\n",
    "        #print(\"xresh shape\", Xresh.shape, diff.shape)\n",
    "        deriv = np.copy(Xresh)\n",
    "        deriv[Xresh > -1.0] = dHflat[Xresh > -1.0]\n",
    "        deriv[Xresh == -1.0] = 0.0\n",
    "        deriv = deriv.reshape(X.shape[0],X.shape[1],h, w)\n",
    "        #print(\"deriv shape max pool = \", deriv.shape)\n",
    "        return deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]] (2, 2, 2)\n",
      "3\n",
      "[[1 2]\n",
      " [3 4]] (2, 2)\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "A = np.array([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
    "print(A, A.shape)\n",
    "print(A[0,1,0])\n",
    "B=np.array([[1,2],[3,4]])\n",
    "print(B, B.shape)\n",
    "print(B[1,0])\n",
    "int(98/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-caa07f00056f>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-caa07f00056f>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    dXmaxpool_test = max, X_arg[1Pool2D_layer2.backward(X_arg, ddH)\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# test Max Pool 2D\n",
    "X = np.zeros((1,1,4,4,1))\n",
    "for i in range(X.shape[2]):\n",
    "    for j in range(X.shape[3]):\n",
    "        X[:,:,i,j,:]= i -2*j \n",
    "x_dummy=X.reshape(4,4)\n",
    "ddH = np.ones((5,5,1))\n",
    "maxPool2D_layer2 = MaxPool2D()\n",
    "pool = np.array([2,2])\n",
    "H, X_arg =maxPool2D_layer2.forward(X, pool)\n",
    "dXmaxpool_test = max, X_arg[1Pool2D_layer2.backward(X_arg, ddH)\n",
    "print(X_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  4.  6.  8. 10.]\n",
      " [ 4.  6.  8. 10. 12.]\n",
      " [ 6.  8. 10. 12. 14.]\n",
      " [ 8. 10. 12. 14. 16.]\n",
      " [10. 12. 14. 16. 18.]]\n"
     ]
    }
   ],
   "source": [
    "print(H.reshape(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\n",
      " [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      " [ 2.  3.  4.  5.  6.  7.  8.  9. 10. 11.]\n",
      " [ 3.  4.  5.  6.  7.  8.  9. 10. 11. 12.]\n",
      " [ 4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      " [ 5.  6.  7.  8.  9. 10. 11. 12. 13. 14.]\n",
      " [ 6.  7.  8.  9. 10. 11. 12. 13. 14. 15.]\n",
      " [ 7.  8.  9. 10. 11. 12. 13. 14. 15. 16.]\n",
      " [ 8.  9. 10. 11. 12. 13. 14. 15. 16. 17.]\n",
      " [ 9. 10. 11. 12. 13. 14. 15. 16. 17. 18.]]\n"
     ]
    }
   ],
   "source": [
    "print(X.reshape(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_arg.reshape(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "def CrossEntropy(yHat, y):\n",
    "    length = yHat.shape[0]\n",
    "    y = y.flatten()\n",
    "    yHat = yHat.flatten()\n",
    "    #print(yHat.shape)\n",
    "    output = np.copy(y)\n",
    "    #print(output.shape)\n",
    "    output[y==1]= -np.log(yHat)[y==1]\n",
    "    output[y==0] = -np.log(1 - yHat)[y==0] # weighting of the classes\n",
    "    #output = -y*np.log(yHat+10**-8)+(1-y)*np.log(1-yhat+10**-8)\n",
    "    output1 = np.sum(np.abs(output))/800\n",
    "    return output1, output\n",
    "def derivCrossEntropy(yHat, y):\n",
    "    dloss = np.zeros((yHat.shape[0]))\n",
    "    for i in range(yHat.shape[0]):\n",
    "        if yHat[i] < 1 and yHat[i] > 0:\n",
    "            dloss[i] = -y[i]/yHat[i] + (1-y[i])/(1-yHat[i])\n",
    "        elif yHat[i] == 1 and y[i] == 1:\n",
    "            dloss[i] = -1\n",
    "        elif yHat[i] == 0 and y[i] == 0:\n",
    "            dloss[i] = 1\n",
    "        elif yHat[i] == 1 and y[i] == 0:\n",
    "            dloss[i] = 2*63\n",
    "        elif yHat[i] == 0 and y[i] == 1:\n",
    "            dloss[i] = -2*63\n",
    "    return dloss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TB done:\n",
    "# write main function calling forward & backward\n",
    "# write flattening (can be done in MAIN)\n",
    "# try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight update\n",
    "class Optimizer():\n",
    "    # initialize training parameters\n",
    "    def __init__(self, W):\n",
    "       # self.Wconv = np.zeros((nb_Wconv1, nb_Wconv2))\n",
    "       # self.Bconv = np.zeros((nb_Bconv))\n",
    "       # self.Wr, self.Wh, self.Wz = np.zeros((nb_GRUin, nb_GRUout)), np.zeros((nb_GRUin, nb_GRUout)), np.zeros((nb_GRUin, nb_GRUout))\n",
    "       # self.Ur, self.Uh, self.Uz = np.zeros((nb_GRUout, nb_GRUout)), np.zeros((nb_GRUout, nb_GRUout)), np.zeros((nb_GRUout, nb_GRUout))\n",
    "       # self.Wlin = np.zeros((nb_Wlin)) # only 1 output\n",
    "        self.alpha = 0.001\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.eps = 10**-7\n",
    "        self.v = np.zeros((W.shape))\n",
    "        self.m = np.zeros((W.shape))\n",
    "        self.n_iters = 1\n",
    "    def adam_update(self, W, dW):\n",
    "        #np.clip(dW, -5, 5, out=dW)\n",
    "        self.m = self.beta1*self.m + (1 - self.beta1)*dW\n",
    "        self.v = self.beta2*self.v + (1 - self.beta2)*np.power(dW, 2)\n",
    "        m_corr = self.m/(1-self.beta1)\n",
    "        v_corr = self.v/(1-self.beta2)\n",
    "        #if self.n_iters % 10 == 0:\n",
    "        #self.alpha = self.alpha/np.sqrt(0.1*self.n_iters+0.9)\n",
    "        W = W  - self.alpha*m_corr/(np.sqrt(v_corr)+self.eps)   \n",
    "        #W = W - self.alpha*dW\n",
    "        self.n_iters += 1\n",
    "        return W\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE sigbufs (23, 921600)\n",
      "921600\n",
      "(18, 1, 1, 23, 921600, 1)\n",
      "(18, 10000, 1)\n",
      "initial time: 0\n",
      "initial time: 0\n",
      "initial time: 700000\n",
      "initial time: 300000\n",
      "initial time: 0\n",
      "initial time: 0\n",
      "initial time: 0\n",
      "initial time: 0\n",
      "(800, 10, 23, 100, 1) (800, 1)\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "\n",
    "# get data\n",
    "np.random.seed(0)\n",
    "output = 1\n",
    "#X=np.array(sigbufs\n",
    "batch_size = 100 #(length)\n",
    "batch_mod = 10000\n",
    "dataset_size = 18\n",
    "X, Y = list(), list()\n",
    "\n",
    "#**********************CALLLING DATA GENERATOR FUNCTION ***********************\n",
    "X, input_size, length = get_sizes(X, dataset_size)\n",
    "print(X.shape)\n",
    "#Defining sizes for input/target data\n",
    "Y=np.zeros((dataset_size, batch_mod, output))\n",
    "#yhat=np.zeros((dataset_size, batch_mod, output))\n",
    "\n",
    "\n",
    "#***********************CALLING TARGET GENERATOR FUNCTION**********************\n",
    "file = 'database/chb01-summary.txt'\n",
    "Y = target_gen(output, batch_mod, dataset_size, batch_size, file)\n",
    "#******************************************************************************\n",
    "print(Y.shape)\n",
    "\n",
    "input_size_new = 23\n",
    "#elements of each frame\n",
    "batch_size_new = 100\n",
    "#time steps equivalent \n",
    "timesteps= 10 # meaning of this?\n",
    "#number of sequences\n",
    "seq_number = 100\n",
    "#Re-defining dataset size for training\n",
    "dataset_size = 8\n",
    "max_iters = 40 #nb_epochs\n",
    "#Defining sizes for input/target data\n",
    "X_new=np.zeros((seq_number*dataset_size, timesteps, input_size_new, batch_size_new, 1))\n",
    "#yhat=np.zeros((dataset_size, batch_mod, output))\n",
    "Y_new=np.zeros((seq_number*dataset_size, output))\n",
    "loss = 0\n",
    "for m in range(0,dataset_size):\n",
    "    if (m == 3-1 or  m == 4-1 or m == 15-1 or m == 16-1 or m == 18-1 ):\n",
    "        if m == 3-1:\n",
    "            initial_time = 700000\n",
    "        if m == 4-1:\n",
    "            initial_time = 300000\n",
    "        if m == 15-1:\n",
    "            initial_time = 400000\n",
    "        if m == 16-1:\n",
    "            initial_time = 200000\n",
    "        if m == 18-1:\n",
    "            initial_time = 400000\n",
    "    else:\n",
    "        initial_time = 0\n",
    "    print('initial time:', initial_time)\n",
    "    for i in range(0,seq_number):\n",
    "        for j in range(0,timesteps):\n",
    "            initial = initial_time+(i*batch_size_new*timesteps)+j*batch_size_new\n",
    "            final = initial_time+(i*batch_size_new*timesteps)+((j+1)*batch_size_new)\n",
    "            #print(initial_time+(i*batch_size_new*timesteps)+j*batch_size_new)\n",
    "            #print(initial_time+(i*batch_size_new*timesteps)+((j+1)*batch_size_new))\n",
    "            X_new[seq_number*m+i,j,:,0:batch_size_new,0] =  X[m,0,0,0:input_size_new,initial:final,0] \n",
    "\n",
    "#standardizing data\n",
    "max_value = np.amax(abs(X_new))\n",
    "min_value = np.amin(abs(X_new))\n",
    "# print('MAX VALUE', max_value)\n",
    "X_new = X_new/max_value # standardization by max value (all values between 0 and 1)\n",
    "#size of data: (800, 10, 23, 100, 1)\n",
    "Y_new[seq_number*2+33*2:seq_number*2+39*2,output-1] = 1\n",
    "Y_new[seq_number*3+37*2:seq_number*3+42*2,output-1] = 1\n",
    "Y_new[seq_number*14+21*2:seq_number*14+27*2,output-1] = 1\n",
    "Y_new[seq_number*15+29*2:seq_number*15+37*2,output-1] = 1\n",
    "\n",
    "\n",
    "print(X_new.shape, Y_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Conv2D layer\n",
      "creating 2D Max pooling layer\n",
      "creating GRU layer\n",
      "(1078, 100)\n",
      "(32, 10, 100)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24]\n",
      "iter 0 --------- loss = 0.26452899889869724\n",
      "iter 1 --------- loss = 0.16052445551142003\n",
      "iter 2 --------- loss = 0.13627155696090995\n",
      "iter 3 --------- loss = 0.13318537704804756\n",
      "iter 4 --------- loss = 0.12406822375387183\n",
      "iter 5 --------- loss = 0.10476236644192134\n",
      "iter 6 --------- loss = 0.09787122677232576\n",
      "iter 7 --------- loss = 0.08346428494484236\n",
      "iter 8 --------- loss = 0.08234919487286066\n",
      "iter 9 --------- loss = 0.06268271452083796\n",
      "iter 10 --------- loss = 0.05105533928941032\n",
      "iter 11 --------- loss = 0.05178323007580848\n",
      "iter 12 --------- loss = 0.04729319395344658\n",
      "iter 13 --------- loss = 0.046674752385038466\n",
      "iter 14 --------- loss = 0.04278710035655143\n",
      "iter 15 --------- loss = 0.0424824804750253\n",
      "iter 16 --------- loss = 0.044656851359780425\n",
      "iter 17 --------- loss = 0.040352072567666895\n",
      "iter 18 --------- loss = 0.04128044534704161\n",
      "iter 19 --------- loss = 0.0404336423970042\n",
      "iter 20 --------- loss = 0.04414904743159877\n",
      "iter 21 --------- loss = 0.03854322103088395\n",
      "iter 22 --------- loss = 0.037114280512957724\n",
      "iter 23 --------- loss = 0.03654463476604884\n",
      "iter 24 --------- loss = 0.03604988401578336\n",
      "iter 25 --------- loss = 0.0363517757643956\n",
      "iter 26 --------- loss = 0.03557838891088858\n",
      "iter 27 --------- loss = 0.03412159196194716\n",
      "iter 28 --------- loss = 0.03875758826489773\n",
      "iter 29 --------- loss = 0.03895336840504413\n",
      "iter 30 --------- loss = 0.03827954860928837\n",
      "iter 31 --------- loss = 0.03377304968992019\n",
      "iter 32 --------- loss = 0.0326422355490213\n",
      "iter 33 --------- loss = 0.029522269149117916\n",
      "iter 34 --------- loss = 0.042165368593735625\n",
      "iter 35 --------- loss = 0.03803949543242426\n",
      "iter 36 --------- loss = 0.04418799932657351\n",
      "iter 37 --------- loss = 0.031023865845121904\n",
      "iter 38 --------- loss = 0.030092797158770085\n",
      "iter 39 --------- loss = 0.03019360835079463\n",
      "iter 40 --------- loss = 0.02871333703456651\n",
      "iter 41 --------- loss = 0.024028261849659543\n",
      "iter 42 --------- loss = 0.023728639917335523\n",
      "iter 43 --------- loss = 0.019524948016392478\n",
      "iter 44 --------- loss = 0.034603990712868044\n",
      "iter 45 --------- loss = 0.02068030863803441\n",
      "iter 46 --------- loss = 0.030502417301238795\n",
      "iter 47 --------- loss = 0.015087447008728811\n",
      "iter 48 --------- loss = 0.005875301167330159\n",
      "iter 49 --------- loss = 0.017316943483512308\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# sequences, timesteps, features, batches. (100 x nb_files, 10,23, 100,,1)\n",
    "# define model\n",
    "# initialize all parameters\n",
    "sgd_batch_size = int(32)\n",
    "layer_0 = Conv2D(2,2,2)\n",
    "# add activation\n",
    "layer_1 = MaxPool2D()\n",
    "#layer_2 = GRU(dataset_size*seq_number,timesteps,1078,100) #sequences, timesteps, features, outputs\n",
    "layer_2 = GRU(sgd_batch_size, timesteps, 1078, 100)\n",
    "#layer_3 = Linear(100,1))\n",
    "#Lin_bias_col = np.ones((800)) # create column of 1's for the bias.\n",
    "#S_GRU_augmented = np.ones((sgd_batch_size,101))\n",
    "# add activation\n",
    "#training phase\n",
    "dataset_size = 8\n",
    "nb_ones =0\n",
    "i = 0\n",
    "order_SGD = np.arange(0, int(dataset_size*seq_number/sgd_batch_size))\n",
    "print(order_SGD)\n",
    "for i in range(Y_new.shape[0]):\n",
    "    if Y_new[i]==1:\n",
    "        nb_ones += 1\n",
    "nb_zeros = 0\n",
    "#forward pass\n",
    "for n_iters in range(50):\n",
    "    full_loss = 0\n",
    "    #pick sequence randomly:\n",
    "    #i = int(randint(200,300)/sgd_batch_size)\n",
    "    np.random.shuffle(order_SGD)\n",
    "    for p in range(int(dataset_size*seq_number/sgd_batch_size)):\n",
    "        i = order_SGD[p]\n",
    "        #print(i)\n",
    "    #conv layer\n",
    "        HConv = layer_0.forward(X_new[i*sgd_batch_size:(i+1)*sgd_batch_size,:,:,:,:]) # 5D data for train_data, 3D for Wconv 2D for Bconv\n",
    "        #print(HConv.shape)\n",
    "        YConv = reLU(HConv, deriv=False) # no requirement on shape\n",
    "        DERIV_PROTECT = np.copy(YConv)\n",
    "        DERIV_PROTECT = DERIV_PROTECT.flatten()\n",
    "        DERIV_PROTECT[DERIV_PROTECT != 0.] = 1\n",
    "        DERIV_PROTECT = DERIV_PROTECT.reshape(-1,10,22,99,2)\n",
    "        #pooling layer\n",
    "        pool_kernel = np.array([2,2])\n",
    "        YPool, XArgmax = layer_1.forward(YConv,  pool_kernel) #5D data for YConv\n",
    "        \n",
    "        #flattening\n",
    "        X_GRU_flat = YPool.reshape(YPool.shape[0],10,-1) # check size here should be 3D (100*nb_files, 10, 1078)\n",
    "        \n",
    "        #GRU\n",
    "        yhat = layer_2.forward(X_GRU_flat)\n",
    "        #last = S_GRU.shape[1]-1 # timesteps\n",
    "        #S_GRU_augmented[:,1:101] =  S_GRU[:,last,:]\n",
    "        #HLinear = layer_3.forward(S_GRU_augmented)\n",
    "        #print(yhat)\n",
    "        #calculate loss\n",
    "        dy = np.zeros((yhat.shape))\n",
    "        temp = np.zeros((yhat.shape[0]))\n",
    "        rep_loss = np.zeros((yhat.shape[0]))\n",
    "        Ytrue = Y_new[i*sgd_batch_size:(i+1)*sgd_batch_size].reshape(-1)\n",
    "            \n",
    "        temp[Ytrue==1] = 2*(yhat-Y_new[i*sgd_batch_size:(i+1)*sgd_batch_size]).reshape(-1)[Ytrue==1]\n",
    "        temp[Ytrue==0] = 2*(yhat-Y_new[i*sgd_batch_size:(i+1)*sgd_batch_size]).reshape(-1)[Ytrue==0]\n",
    "        dy=temp.reshape(-1,1)\n",
    "        \n",
    "\n",
    "        #********************************CLASS WEIGHTING************\n",
    "        #for k in range(yhat.shape[0]):\n",
    "         #   if Y_new[i*sgd_batch_size+k,:]==0:\n",
    "          #      dhlin[k,:] = 2*(yhat[k,:] - Y_new[i*sgd_batch_size+k,:])\n",
    "           # else:\n",
    "            #    dhlin[k,:] = 20*(yhat[k,:] - Y_new[i*sgd_batch_size+k,:])\n",
    "        #only 0.5/-0.5\n",
    "        #dxlin, dwlin = layer_3.backward(dhlin, S_GRU_augmented)\n",
    "        #dxlin = np.delete(dxlin, 0, 1)\n",
    "\n",
    "        dsGRU, dxGRU = layer_2.backward(dy, X_GRU_flat)\n",
    "        dyMaxPool = dxGRU.reshape(dxGRU.shape[0], dxGRU.shape[1],11,49,2)\n",
    "\n",
    "        dxMaxPool = layer_1.backward(XArgmax, dyMaxPool)\n",
    "\n",
    "        dxMaxPool_augmented = np.zeros((dxMaxPool.shape[0], dxMaxPool.shape[1], dxMaxPool.shape[2], dxMaxPool.shape[3]+1, dxMaxPool.shape[4]))\n",
    "        dxMaxPool_augmented[:,:,:,0:dxMaxPool.shape[3],:]=dxMaxPool*DERIV_PROTECT[:,:,:,0:dxMaxPool.shape[3],:]\n",
    "        dhConv2D = reLU(HConv, deriv=True)*dxMaxPool_augmented\n",
    "        #print(dhConv2D.shape)\n",
    "        layer_0.backward(dhConv2D, X_new[i*sgd_batch_size:(i+1)*sgd_batch_size,:,:,:,:])\n",
    "    \n",
    "        loss, __ = CrossEntropy(yhat, Y_new[i*sgd_batch_size:(i+1)*sgd_batch_size]) # works only for y = 0 or 1\n",
    "        full_loss += loss\n",
    "    #print(full_loss, dataset_size)\n",
    "    #full_loss = full_loss/int(dataset_size/32)\n",
    "    print(\"iter\", n_iters, \"---------\", \"loss =\", full_loss)\n",
    "    \n",
    "    #wz, dwz,_,_,_,_,_,_,_,_,_,_ = layer_2.get_parameters()\n",
    "    #print(wz[580:585,0:10], dwz[580:585,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(yhat,y):\n",
    "    err = 0\n",
    "    yhat = np.round(yhat)\n",
    "    diff = yhat-y\n",
    "    for i in range(diff.shape[0]):\n",
    "        if diff != 0:\n",
    "            err+=1\n",
    "    err = err/diff.shape[0]\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial time: 800000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#*******************************test phase***********************************************\n",
    "startfile = 16\n",
    "nb_files = 1\n",
    "seq_number=100\n",
    "#Defining sizes for input/target data\n",
    "X_test=np.zeros((seq_number*nb_files, timesteps, input_size_new, batch_size_new, 1))\n",
    "\n",
    "layer_2.change_input_size(seq_number*nb_files,10,100)\n",
    "X_GRU_flat_augmented_test = np.ones((seq_number*nb_files,10,1079))\n",
    "S_GRU_augmented_test = np.ones((seq_number*nb_files,101))\n",
    "\n",
    "#for i in range(0,seq_number):\n",
    "   # for j in range(0,timesteps):\n",
    "    #    #print(initial, final, batch_size_new, file-1)\n",
    "    #    initial = initial_time+(i*batch_size_new*timesteps)+j*batch_size_new\n",
    "    #    final = initial_time+(i*batch_size_new*timesteps)+((j+1)*batch_size_new)\n",
    "        #print(initial_time+(i*batch_size_new*timesteps)+j*batch_size_new)\n",
    "        #print(initial_time+(i*batch_size_new*timesteps)+((j+1)*batch_size_new))\n",
    "     #   X_test[i,j,:,0:batch_size_new,0] =  X[file-1,0,0,0:input_size_new,initial:final,0] \n",
    "for m in range(startfile,nb_files+startfile):\n",
    "    if (m == 3-1 or  m == 4-1 or m == 15-1 or m == 16-1 or m == 18-1 ):\n",
    "        if m == 3-1:\n",
    "            initial_time = 700000\n",
    "        if m == 4-1:\n",
    "            initial_time = 300000\n",
    "        if m == 15-1:\n",
    "            initial_time = 400000\n",
    "        if m == 16-1:\n",
    "            initial_time = 200000\n",
    "        if m == 18-1:\n",
    "            initial_time = 400000\n",
    "    else:\n",
    "        initial_time = 800000\n",
    "    print('initial time:', initial_time)\n",
    "    for i in range(0,seq_number):\n",
    "        for j in range(0,timesteps):\n",
    "            initial = initial_time+(i*batch_size_new*timesteps)+j*batch_size_new\n",
    "            final = initial_time+(i*batch_size_new*timesteps)+((j+1)*batch_size_new)\n",
    "            #print(initial_time+(i*batch_size_new*timesteps)+j*batch_size_new)\n",
    "            #print(initial_time+(i*batch_size_new*timesteps)+((j+1)*batch_size_new))\n",
    "            X_test[seq_number*m+i-startfile*seq_number,j,:,0:batch_size_new,0] =  X[m,0,0,0:input_size_new,initial:final,0]    \n",
    "\n",
    "   \n",
    "max_value_2 = np.amax(abs(X_test))\n",
    "min_value = np.amin(abs(X_test))\n",
    "# print('MAX VALUE', max_value)\n",
    "X_test = X_test/max_value_2\n",
    "\n",
    "#generate predictions:\n",
    "HConv = layer_0.forward(X_test) # 5D data for train_data, 3D for Wconv 2D for Bconv\n",
    "YConv = reLU(HConv, deriv=False) # no requirement on shape\n",
    "#pooling layer\n",
    "pool_kernel = np.array([2,2])\n",
    "YPool, XArgmax = layer_1.forward(YConv,  pool_kernel) #5D data for YConv\n",
    "#flattening\n",
    "X_GRU_flat = YPool.reshape(YPool.shape[0],10,-1) # check size here should be 3D (100*nb_files, 10, 1078)\n",
    "#print(X_GRU_flat.shape)\n",
    "#GRU\n",
    "yhat_test = layer_2.forward(X_GRU_flat) # timesteps\n",
    "\n",
    "file2 = open(\"CNN_GRU_testsetfile18_lowlevel_seed1\", 'w')\n",
    "np.savetxt(file2, yhat_test, delimiter=\",\" )\n",
    "file2.close()\n",
    "#test_err = compute_error(yhat_test, Y_test)\n",
    "#print(test_err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round([1.5,1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2],[3,4]])\n",
    "I = np.tile(A, (2,2,2))\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.01349721 -0.79802286]\n",
      "  [-0.01274448 -1.14339606]]\n",
      "\n",
      " [[ 0.00696058 -0.66652883]\n",
      "  [ 0.01983425 -0.17549277]]]\n",
      "[[-0.29894161 -1.45337415]]\n"
     ]
    }
   ],
   "source": [
    "print(layer_0.W)\n",
    "print(layer_0.B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fileY16 = open(\"Ytruth16.txt\", 'w')\n",
    "np.savetxt(fileY16, (Y_new[seq_number*15:seq_number*16,:]), delimiter=\",\", fmt='%.1f')\n",
    "fileY16.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03125 (1600, 1)\n"
     ]
    }
   ],
   "source": [
    "nb_ones =0\n",
    "for i in range(Y_new.shape[0]):\n",
    "    if Y_new[i]==1:\n",
    "        nb_ones +=1\n",
    "nb_ones/=1600\n",
    "print(nb_ones, Y_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pow2_ternarization(W):\n",
    "    nbits = 16\n",
    "    nbfrac = 13\n",
    "    nbint = nbits-nbfrac\n",
    "    shape = W.shape\n",
    "    W = W.flatten()\n",
    "    Wclip = np.copy(W)\n",
    "    Wclip[W <= -2**nbint]=-2**nbint\n",
    "    Wclip[W >= 2**nbint]=2**nbint\n",
    "    Wclip = Wclip.reshape(shape)\n",
    "    Wclip = np.round(Wclip*2**nbfrac)*2**-nbfrac\n",
    "    return Wclip\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
